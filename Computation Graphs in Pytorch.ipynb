{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "DDJwQPZcupab",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "# LIS 640 Applied Deep Learning : Computation Graphs in Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzIKo3_p9WN8"
      },
      "source": [
        "# Code Blocks for Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hs9lnGfQ9WN9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from utils import svm_loss, softmax_loss\n",
        "\n",
        "def hello_fully_connected_networks():\n",
        "    print('Hello from problem3.ipynb!')\n",
        "\n",
        "\n",
        "class Linear(object):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(x, w):\n",
        "        \"\"\"\n",
        "        Computes the forward pass for an linear (fully-connected) layer.\n",
        "        The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
        "        examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
        "        reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
        "        then transform it to an output vector of dimension M.\n",
        "        Inputs:\n",
        "        - x: A tensor containing input data, of shape (N, d_1, ..., d_k)\n",
        "        - w: A tensor of weights, of shape (D, M)\n",
        "        Returns a tuple of:\n",
        "        - out: output, of shape (N, M)\n",
        "        - cache: (x, w)\n",
        "        \"\"\"\n",
        "        out = None\n",
        "        out = x.view(x.shape[0],-1).mm(w)\n",
        "        cache = (x, w)\n",
        "        return out, cache\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(dout, cache):\n",
        "        \"\"\"\n",
        "        Computes the backward pass for an linear layer.\n",
        "        Inputs:\n",
        "        - dout: Upstream derivative, of shape (N, M)\n",
        "        - cache: Tuple of:\n",
        "          - x: Input data, of shape (N, d_1, ... d_k)\n",
        "          - w: Weights, of shape (D, M)\n",
        "        Returns a tuple of:\n",
        "        - dx: Gradient with respect to x, of shape\n",
        "          (N, d1, ..., d_k)\n",
        "        - dw: Gradient with respect to w, of shape (D, M)\n",
        "        \"\"\"\n",
        "        x, w = cache\n",
        "        dx, dw = None, None\n",
        "        dx = dout.mm(w.t()).view(x.shape)\n",
        "        dw = x.view(x.shape[0],-1).t().mm(dout)\n",
        "        return dx, dw\n",
        "\n",
        "\n",
        "class ReLU(object):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(x):\n",
        "        \"\"\"\n",
        "        Computes the forward pass for a layer of rectified\n",
        "        linear units (ReLUs).\n",
        "        Input:\n",
        "        - x: Input; a tensor of any shape\n",
        "        Returns a tuple of:\n",
        "        - out: Output, a tensor of the same shape as x\n",
        "        - cache: x\n",
        "        \"\"\"\n",
        "        out = None\n",
        "        ###################################################\n",
        "        # TODO: Implement the ReLU forward pass.          #\n",
        "        # You should not change the input tensor x with an#\n",
        "        # in-place operation. Try to clone it first.      #\n",
        "        ###################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        out = torch.clamp(x, min=0)\n",
        "        ###################################################\n",
        "        #                 END OF YOUR CODE                #\n",
        "        ###################################################\n",
        "        cache = x\n",
        "        return out, cache\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(dout, cache):\n",
        "        \"\"\"\n",
        "        Computes the backward pass for a layer of rectified\n",
        "        linear units (ReLUs).\n",
        "        Input:\n",
        "        - dout: Upstream derivatives, of any shape\n",
        "        - cache: Input x, of same shape as dout\n",
        "        Returns:\n",
        "        - dx: Gradient with respect to x\n",
        "        \"\"\"\n",
        "        dx, x = None, cache\n",
        "        #####################################################\n",
        "        # TODO: Implement the ReLU backward pass.           #\n",
        "        # You should not change the input tensor dout with  #\n",
        "        # an in-place operation. Try to clone it first.     #\n",
        "        #####################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        dx = dout.clone()\n",
        "        dx[x<=0] = 0\n",
        "        #####################################################\n",
        "        #                  END OF YOUR CODE                 #\n",
        "        #####################################################\n",
        "        return dx\n",
        "\n",
        "\n",
        "class TwoLayerNet(object):\n",
        "    \"\"\"\n",
        "    A two-layer fully-connected neural network with ReLU nonlinearity and\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "    The architecure should be linear - relu - linear - softmax.\n",
        "    Note that this class does not implement gradient descent; instead, it\n",
        "    will interact with a separate Solver object that is responsible for running\n",
        "    optimization.\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to PyTorch tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=3*32*32, hidden_dim=100, num_classes=10,\n",
        "                 weight_scale=1e-3, reg=0.0,\n",
        "                 dtype=torch.float32, device='cpu'):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "        Inputs:\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        - reg: Scalar giving L2 regularization strength.\n",
        "        - dtype: A torch data type object; all computations will be\n",
        "          performed using this datatype. float is faster but less accurate,\n",
        "          so you should use double for numeric gradient checking.\n",
        "        - device: device to use for computation. 'cpu' or 'cuda'\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        self.reg = reg\n",
        "\n",
        "        # Initialize\n",
        "        self.params['W1'] = torch.zeros(input_dim, hidden_dim, dtype=dtype,device = device)\n",
        "        self.params['W1'] += weight_scale*torch.randn(input_dim, hidden_dim, dtype=dtype,device= device)\n",
        "        self.params['W2'] = torch.zeros(hidden_dim, num_classes, dtype=dtype,device= device)\n",
        "        self.params['W2'] += weight_scale*torch.randn(hidden_dim, num_classes, dtype=dtype,device= device)\n",
        "\n",
        "    def save(self, path):\n",
        "        checkpoint = {\n",
        "          'reg': self.reg,\n",
        "          'params': self.params,\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, path)\n",
        "        print(\"Saved in {}\".format(path))\n",
        "\n",
        "    def load(self, path, dtype, device):\n",
        "        checkpoint = torch.load(path, map_location='cpu')\n",
        "        self.params = checkpoint['params']\n",
        "        self.reg = checkpoint['reg']\n",
        "        for p in self.params:\n",
        "            self.params[p] = self.params[p].type(dtype).to(device)\n",
        "        print(\"load checkpoint file: {}\".format(path))\n",
        "\n",
        "    def loss(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute loss and gradient for a minibatch of data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Tensor of input data of shape (N, d_1, ..., d_k)\n",
        "        - y: int64 Tensor of labels, of shape (N,). y[i] gives the\n",
        "          label for X[i].\n",
        "\n",
        "        Returns:\n",
        "        If y is None, then run a test-time forward pass of the model\n",
        "        and return:\n",
        "        - scores: Tensor of shape (N, C) giving classification scores,\n",
        "          where scores[i, c] is the classification score for X[i]\n",
        "          and class c.\n",
        "        If y is not None, then run a training-time forward and backward\n",
        "        pass and return a tuple of:\n",
        "        - loss: Scalar value giving the loss\n",
        "        - grads: Dictionary with the same keys as self.params, mapping\n",
        "          parameter names to gradients of the loss with respect to\n",
        "          those parameters.\n",
        "        \"\"\"\n",
        "        scores = None\n",
        "        #############################################################\n",
        "        # TODO: Implement the forward pass for the two-layer net,   #\n",
        "        # computing the class scores for X and storing them in the  #\n",
        "        # scores variable.                                          #\n",
        "        #############################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        hidden = X.mm(self.params['W1'])\n",
        "        hidden = torch.nn.functional.relu(hidden)\n",
        "        scores = hidden.mm(self.params['W2'])\n",
        "        ##############################################################\n",
        "        #                     END OF YOUR CODE                       #\n",
        "        ##############################################################\n",
        "\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0, {}\n",
        "\n",
        "        # compute the loss and gradient for softmax classification\n",
        "        loss, dout = softmax_loss(scores, y)\n",
        "        ###################################################################\n",
        "        # TODO: Implement the backward pass for the two-layer net.        #\n",
        "        # The upstream derivatives \"dout\" have been given.                #\n",
        "        # You just need to compute gradients of Linear and ReLU layer     #\n",
        "        ###################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        grads['W2'] = hidden.t().mm(dout)\n",
        "\n",
        "        # Gradient of hidden layer (ReLU)\n",
        "        dhidden = dout.mm(self.params['W2'].t())\n",
        "        dhidden_relu = dhidden * (hidden > 0).type(dhidden.dtype)\n",
        "\n",
        "        # Gradient of W1\n",
        "        grads['W1'] = X.t().mm(dhidden_relu)\n",
        "\n",
        "        # Regularization gradient\n",
        "        grads['W1'] += self.reg * self.params['W1']\n",
        "        grads['W2'] += self.reg * self.params['W2']\n",
        "        ###################################################################\n",
        "        #                     END OF YOUR CODE                            #\n",
        "        ###################################################################\n",
        "\n",
        "        return loss, grads\n",
        "\n",
        "\n",
        "def sgd(w, dw, config=None):\n",
        "    \"\"\"\n",
        "    Performs vanilla stochastic gradient descent.\n",
        "    config format:\n",
        "    - learning_rate: Scalar learning rate.\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = {}\n",
        "    config.setdefault('learning_rate', 1e-2)\n",
        "\n",
        "    w -= config['learning_rate'] * dw\n",
        "    return w, config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpsJ8HDU9WN_"
      },
      "source": [
        "# Questions for Problem 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wusc3R8O9WN_"
      },
      "source": [
        "# Set up code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RnHXeQok9WN_"
      },
      "outputs": [],
      "source": [
        "import utils\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "3Qiu9_4pe1CP",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "tags": [
          "pdf-ignore"
        ]
      },
      "source": [
        "In this exercise we will implement two-layer network using a more modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
        "\n",
        "```python\n",
        "def forward(x, w):\n",
        "  \"\"\" Receive inputs x and weights w \"\"\"\n",
        "  # Do some computations ...\n",
        "  z = # ... some intermediate value\n",
        "  # Do some more computations ...\n",
        "  out = # the output\n",
        "   \n",
        "  cache = (x, w, z, out) # Values we need to compute gradients\n",
        "   \n",
        "  return out, cache\n",
        "```\n",
        "\n",
        "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
        "\n",
        "```python\n",
        "def backward(dout, cache):\n",
        "  \"\"\"\n",
        "  Receive dout (derivative of loss with respect to outputs) and cache,\n",
        "  and compute derivative with respect to inputs.\n",
        "  \"\"\"\n",
        "  # Unpack cache values\n",
        "  x, w, z, out = cache\n",
        "  \n",
        "  # Use values in cache to compute derivatives\n",
        "  dx = # Derivative of loss with respect to x\n",
        "  dw = # Derivative of loss with respect to w\n",
        "  \n",
        "  return dx, dw\n",
        "```\n",
        "\n",
        "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures. Your task here is to implement `ReLU` activation function with modular approach.\n",
        "\n",
        "\n",
        "To validate our implementation, we will compare the analytically computed gradients with numerical approximations of the gradient as done in previous assignments. You can inspect the numeric gradient function `utils.compute_numeric_gradient`. Please note that we have updated the function to accept upstream gradients to allow us to debug intermediate layers easily.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "bdIqQzqiJQE6",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "# ReLU activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "YdX98A_qvTRt",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "We will now implement the ReLU activation function. As above, we will define a class with two empty static methods, and implement them in upcoming cells. The class structure can be found in **Code Blocks for Problem 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "n2DyqL4Ae1Cl",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "## ReLU activation: forward\n",
        "Implement the forward pass for the ReLU activation function in the `ReLU.forward` function. You **should not** change the input tensor with an in-place operation.\n",
        "\n",
        "Run the following to test your implementation of the ReLU forward pass. Your errors should be less than `1e-7`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "QblpieUJe1Cm",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "248c3b85-9bb5-4b55-ca16-8a78dea5a18a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing ReLU.forward function:\n",
            "difference:  4.5454545613554664e-09\n"
          ]
        }
      ],
      "source": [
        "utils.reset_seed(0)\n",
        "x = torch.linspace(-0.5, 0.5, steps=12, dtype=torch.float64, device='cuda')\n",
        "x = x.reshape(3, 4)\n",
        "\n",
        "out, _ = ReLU.forward(x)\n",
        "correct_out = torch.tensor([[ 0.,          0.,          0.,          0.,        ],\n",
        "                            [ 0.,          0.,          0.04545455,  0.13636364,],\n",
        "                            [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]],\n",
        "                            dtype=torch.float64,\n",
        "                            device='cuda')\n",
        "\n",
        "# Compare your output with ours. The error should be on the order of e-8\n",
        "print('Testing ReLU.forward function:')\n",
        "print('difference: ', utils.rel_error(out, correct_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "3bSInb7xe1Cq",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "## ReLU activation: backward\n",
        "Now implement the backward pass for the ReLU activation function.\n",
        "\n",
        "Again, you should not change the input tensor with an in-place operation.\n",
        "\n",
        "Run the following to test your implementation of `ReLU.backward`. Your errors should be less than `1e-8`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "odiV48zBe1Cr",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf2fa862-c693-43d3-bd1d-893bf5da8f3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing ReLU.backward function:\n",
            "dx error:  2.6317796097761553e-10\n"
          ]
        }
      ],
      "source": [
        "utils.reset_seed(0)\n",
        "x = torch.randn(10, 10, dtype=torch.float64, device='cuda')\n",
        "dout = torch.randn(*x.shape, dtype=torch.float64, device='cuda')\n",
        "\n",
        "dx_num = utils.compute_numeric_gradient(lambda x: ReLU.forward(x)[0], x, dout)\n",
        "\n",
        "_, cache = ReLU.forward(x)\n",
        "dx = ReLU.backward(dout, cache)\n",
        "\n",
        "# The error should be on the order of e-12\n",
        "print('Testing ReLU.backward function:')\n",
        "print('dx error: ', utils.rel_error(dx_num, dx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "qq7-cyfQe1C4",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "# Two-layer network\n",
        "In the previous problem2 you implemented a two-layer neural network in a single monolithic class. Now that you have implemented modular versions of the necessary layers, you will reimplement the two layer network using these modular implementations.\n",
        "\n",
        "Complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API.\n",
        "\n",
        "Once you have finished implementing the forward and backward passes of your two-layer net, run the following to test your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "d3JOcfyze1C5",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2bf8b78-ab54-42cc-c5bc-cc1bd9dd4187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing initialization ... \n",
            "Testing test-time forward pass ... \n",
            "Testing training loss (no regularization)\n",
            "Running numeric gradient check:\n",
            "W1 relative error: 1.70e-07\n",
            "W2 relative error: 2.19e-09\n"
          ]
        }
      ],
      "source": [
        "torch.set_printoptions(precision=12, threshold=None, edgeitems=None, linewidth=None, profile=None)\n",
        "\n",
        "utils.reset_seed(0)\n",
        "N, D, H, C = 3, 5, 50, 7\n",
        "X = torch.randn(N, D, dtype=torch.float64, device='cuda')\n",
        "y = torch.randint(C, size=(N,), dtype=torch.int64, device='cuda')\n",
        "\n",
        "std = 1e-3\n",
        "model = TwoLayerNet(\n",
        "          input_dim=D,\n",
        "          hidden_dim=H,\n",
        "          num_classes=C,\n",
        "          weight_scale=std,\n",
        "          dtype=torch.float64,\n",
        "          device='cuda'\n",
        "        )\n",
        "\n",
        "print('Testing initialization ... ')\n",
        "W1_std = torch.abs(model.params['W1'].std() - std)\n",
        "W2_std = torch.abs(model.params['W2'].std() - std)\n",
        "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
        "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
        "\n",
        "print('Testing test-time forward pass ... ')\n",
        "model.params['W1'] = torch.linspace(-0.7, 0.3, steps=D * H, dtype=torch.float64, device='cuda').reshape(D, H)\n",
        "model.params['W2'] = torch.linspace(-0.3, 0.4, steps=H * C, dtype=torch.float64, device='cuda').reshape(H, C)\n",
        "X = torch.linspace(-5.5, 4.5, steps=N * D, dtype=torch.float64, device='cuda').reshape(D, N).t()\n",
        "scores = model.loss(X)\n",
        "correct_scores = torch.tensor(\n",
        "        [[ 8.56847057,  9.12177260,  9.67507463, 10.22837667, 10.78167870,\n",
        "         11.33498073, 11.88828277],\n",
        "        [ 9.09451046,  9.57617926, 10.05784805, 10.53951685, 11.02118564,\n",
        "         11.50285444, 11.98452323],\n",
        "        [ 9.62055036, 10.03058591, 10.44062147, 10.85065703, 11.26069259,\n",
        "         11.67072814, 12.08076370]],\n",
        "    dtype=torch.float64, device='cuda')\n",
        "scores_diff = torch.abs(scores - correct_scores).sum()\n",
        "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
        "\n",
        "print('Testing training loss (no regularization)')\n",
        "y = torch.tensor([0, 5, 1])\n",
        "loss, grads = model.loss(X, y)\n",
        "correct_loss = 2.881451052641\n",
        "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
        "\n",
        "# Errors should be around e-6 or less\n",
        "print('Running numeric gradient check:')\n",
        "loss, grads = model.loss(X, y)\n",
        "\n",
        "for name in sorted(grads):\n",
        "  f = lambda _: model.loss(X, y)[0]\n",
        "  grad_num = utils.compute_numeric_gradient(f, model.params[name])\n",
        "  print('%s relative error: %.2e' % (name, utils.rel_error(grad_num, grads[name])))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
